# Firecrawl Scraper Update - Using /v2/scrape Endpoint

## ğŸ“ Summary

Updated `backend/src/modules/firecrawl_scraper.py` to use Firecrawl's **`/v2/scrape` endpoint** instead of `/v2/crawl`. This provides better single-page scraping with cleaner output.

---

## âœ¨ Key Changes

### **Before (Using `/v2/crawl`):**
```python
payload = {
    "url": target_url,
    "sitemap": "include",
    "crawlEntireDomain": False,
    "limit": 10,
    "scrapeOptions": {
        "onlyMainContent": False,
        "maxAge": 172800000,
        "parsers": ["pdf"],
        "formats": ["markdown", "html"]
    }
}
response = requests.post(f"{base_url}/crawl", json=payload, headers=headers)
```

### **After (Using `/v2/scrape`):**
```python
payload = {
    "url": target_url,
    "onlyMainContent": False,
    "maxAge": 172800000,
    "parsers": ["pdf"],
    "formats": ["markdown", "html"]
}
response = requests.post(f"{base_url}/scrape", json=payload, headers=headers)
```

---

## ğŸ”§ API Method Changes

| Aspect | `/v2/crawl` | `/v2/scrape` |
|--------|----------|----------|
| **Use Case** | Multiple pages | Single page |
| **Parameters** | `url`, `limit`, `sitemap`, `crawlEntireDomain` | `url` |
| **Response** | Array of pages | Single page object |
| **Processing** | Crawls domain/sitemap | Direct extraction |
| **Speed** | Slower (multi-page) | Faster (single) |
| **Cost** | Higher (multiple pages) | Lower (single) |

---

## ğŸ“¦ Updated Functions

### **New Function: `scrape_single_page()`**
```python
def scrape_single_page(
    target_url: str,
    only_main_content: bool = False,
    include_pdf: bool = True
) -> Dict[str, Any]
```
- Uses `/v2/scrape` endpoint
- Returns single page content
- Includes fallback error handling

### **Modified Function: `crawl_website()`**
```python
def crawl_website(target_url, limit=10, ...) -> List[Dict]
```
- Now internally uses `/v2/scrape`
- Simplified payload structure
- Better response format handling

### **Updated Function: `scrape_nintendo_website()`**
- Now uses `scrape_single_page()` for all URL scraping
- Improved logging with âœ“/âœ— indicators
- Better fallback handling (HTTP GET)
- Cleaner additional URL processing

---

## ğŸ”„ Call Flow

```
User calls scrape_nintendo_website()
    â†“
scraper.crawl_website() 
    â†“
scraper.scrape_single_page()  [NEW]
    â†“
POST https://api.firecrawl.dev/v2/scrape
    â†“
Extract and format response
    â†“
Return List[Dict] with pages
```

---

## âœ… Benefits

1. **Cleaner API**: Fewer parameters, more straightforward
2. **Faster**: Single-page scraping is quicker
3. **Better Error Handling**: Improved response parsing
4. **Flexible**: Supports both single and multiple URLs
5. **Robust Fallback**: Uses HTTP GET if Firecrawl fails
6. **Improved Logging**: âœ“ for success, âœ— for failures

---

## ğŸ§ª Example Usage

```python
from backend.src.modules.firecrawl_scraper import scrape_nintendo_website

# Scrape Nintendo homepage + additional URLs
docs = scrape_nintendo_website(
    api_key="your_api_key",
    target_url="https://www.nintendo.com/us/",
    additional_urls=[
        "https://www.nintendo.com/us/switch/",
        "https://www.nintendo.com/us/switch/tech-specs/"
    ]
)

for doc in docs:
    print(f"Title: {doc['title']}")
    print(f"Content preview: {doc['content'][:100]}...")
```

---

## ğŸ“Š Response Handling

The updated scraper now handles:
- âœ… Wrapped responses: `{"data": {...}}`
- âœ… List responses: `[{...}, {...}]`
- âœ… Direct responses: `{...}`
- âœ… HTTP fallback when Firecrawl fails
- âœ… Markdown format preference

---

## ğŸ”‘ Configuration

**Payload Parameters (Same as Your Example):**
```python
{
    "url": "https://example.com",
    "onlyMainContent": False,
    "maxAge": 172800000,        # 48 hours
    "parsers": ["pdf"],
    "formats": ["markdown", "html"]
}
```

---

## âœ”ï¸ Testing

Syntax check: âœ“ Passed
- No import errors
- All methods callable
- Backward compatible with existing code

---

## ğŸ“ Next Steps

1. Run initialization to test: `POST /api/initialize`
2. Verify scraped content: `GET /api/stats`
3. Query chatbot: `POST /api/query`

---

## ğŸ“Œ Notes

- Single page at a time (use loop for multiple pages)
- Respects rate limits with timeouts
- Automatic HTTP fallback if API fails
- Maintains same output format for compatibility

